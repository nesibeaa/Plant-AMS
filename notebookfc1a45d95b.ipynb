{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-28T15:50:27.860049Z",
     "iopub.status.busy": "2025-08-28T15:50:27.859890Z",
     "iopub.status.idle": "2025-08-28T15:50:44.838437Z",
     "shell.execute_reply": "2025-08-28T15:50:44.837896Z",
     "shell.execute_reply.started": "2025-08-28T15:50:27.860032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. IMPORTS AND SETUP\n",
    "# ===============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T15:50:44.839378Z",
     "iopub.status.busy": "2025-08-28T15:50:44.839081Z",
     "iopub.status.idle": "2025-08-28T15:51:51.917011Z",
     "shell.execute_reply": "2025-08-28T15:51:51.916063Z",
     "shell.execute_reply.started": "2025-08-28T15:50:44.839361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2. DATA DOWNLOAD AND SETUP\n",
    "# ===============================\n",
    "\n",
    "# Download dataset (run once)\n",
    "!git clone https://github.com/spMohanty/PlantVillage-Dataset.git\n",
    "data_dir = 'PlantVillage-Dataset/raw/color'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T15:51:51.919357Z",
     "iopub.status.busy": "2025-08-28T15:51:51.919119Z",
     "iopub.status.idle": "2025-08-28T15:51:51.925553Z",
     "shell.execute_reply": "2025-08-28T15:51:51.925016Z",
     "shell.execute_reply.started": "2025-08-28T15:51:51.919334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3. DATA PROCESSING FUNCTIONS\n",
    "# ===============================\n",
    "\n",
    "def define_paths(data_dir):\n",
    "    \"\"\"Veri setindeki t√ºm dosya yollarƒ±nƒ± ve etiketlerini toplar\"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    for fold in os.listdir(data_dir):\n",
    "        foldpath = os.path.join(data_dir, fold)\n",
    "        if os.path.isdir(foldpath):\n",
    "            for file in os.listdir(foldpath):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    filepaths.append(os.path.join(foldpath, file))\n",
    "                    labels.append(fold)\n",
    "    return pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "\n",
    "def split_df(df):\n",
    "    \"\"\"Veri setini train/val/test olarak b√∂ler\"\"\"\n",
    "    train_df, dummy_df = train_test_split(df, train_size=0.8, stratify=df['labels'], random_state=42)\n",
    "    val_df, test_df = train_test_split(dummy_df, train_size=0.5, stratify=dummy_df['labels'], random_state=42)\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T15:51:51.926650Z",
     "iopub.status.busy": "2025-08-28T15:51:51.926390Z",
     "iopub.status.idle": "2025-08-28T15:51:53.234085Z",
     "shell.execute_reply": "2025-08-28T15:51:53.233476Z",
     "shell.execute_reply.started": "2025-08-28T15:51:51.926624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4. DATASET CLASSES\n",
    "# ===============================\n",
    "\n",
    "class PlantMultiOutputDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        # Bitki t√ºrleri ve durumlarƒ±\n",
    "        self.plant_names = sorted(set(label.split(\"___\")[0] for label in dataframe['labels']))\n",
    "        self.status_names = sorted(set(label.split(\"___\")[1] for label in dataframe['labels']))\n",
    "        self.plant_map = {name: idx for idx, name in enumerate(self.plant_names)}\n",
    "        self.status_map = {name.lower(): idx for idx, name in enumerate(self.status_names)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row.filepaths).convert(\"RGB\")\n",
    "        plant_str, status_str = row.labels.split(\"___\")\n",
    "        plant_label = self.plant_map[plant_str]\n",
    "        status_label = self.status_map[status_str.lower()]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=np.array(img))['image']\n",
    "        \n",
    "        return img, torch.tensor(plant_label), torch.tensor(status_label)\n",
    "\n",
    "class PlantOnlyDataset(Dataset):\n",
    "    def __init__(self, dataframe, plant_map, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.plant_map = plant_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row.filepaths).convert(\"RGB\")\n",
    "        plant_str = row.labels.split(\"___\")[0]\n",
    "        plant_label = self.plant_map[plant_str]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=np.array(img))['image']\n",
    "        \n",
    "        return img, torch.tensor(plant_label)\n",
    "\n",
    "class HealthOnlyDataset(Dataset):\n",
    "    def __init__(self, dataframe, status_map, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.status_map = status_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row.filepaths).convert(\"RGB\")\n",
    "        status_str = row.labels.split(\"___\")[1]\n",
    "        status_label = self.status_map[status_str.lower()]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=np.array(img))['image']\n",
    "        \n",
    "        return img, torch.tensor(status_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T15:51:53.235103Z",
     "iopub.status.busy": "2025-08-28T15:51:53.234893Z",
     "iopub.status.idle": "2025-08-28T15:51:54.861311Z",
     "shell.execute_reply": "2025-08-28T15:51:54.860522Z",
     "shell.execute_reply.started": "2025-08-28T15:51:53.235085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5. MODEL ARCHITECTURES\n",
    "# ===============================\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, plant_output_dim, status_output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        # Dropout ve batch normalization ekleyelim (Overfitting √∂nleme i√ßin dropout=0.5)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.fc_plant = nn.Linear(512, plant_output_dim)\n",
    "        self.fc_health = nn.Linear(512, status_output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = self.bn(features)\n",
    "        features = self.dropout(features)\n",
    "        return self.fc_plant(features), self.fc_health(features)\n",
    "\n",
    "class SingleOutputModel(nn.Module):\n",
    "    def __init__(self, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        # Son katmanƒ± deƒüi≈ütir (Overfitting √∂nleme i√ßin dropout=0.5)\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:09:32.363893Z",
     "iopub.status.busy": "2025-08-28T16:09:32.363627Z",
     "iopub.status.idle": "2025-08-28T16:09:32.375283Z",
     "shell.execute_reply": "2025-08-28T16:09:32.374711Z",
     "shell.execute_reply.started": "2025-08-28T16:09:32.363872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6. TRAINING FUNCTIONS\n",
    "# ===============================\n",
    "\n",
    "def train_epoch_multi(model, loader, optimizer, criterion, device, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    plant_correct = 0\n",
    "    health_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y_plant, y_health in tqdm(loader, desc=\"Eƒüitim - Multi\"):\n",
    "        x, y_plant, y_health = x.to(device), y_plant.to(device), y_health.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_plant, out_health = model(x)\n",
    "        loss_plant = criterion(out_plant, y_plant)\n",
    "        loss_health = criterion(out_health, y_health)\n",
    "        loss = loss_plant + loss_health\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        plant_correct += (out_plant.argmax(1) == y_plant).sum().item()\n",
    "        health_correct += (out_health.argmax(1) == y_health).sum().item()\n",
    "        total_samples += y_plant.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    plant_acc = plant_correct / total_samples\n",
    "    health_acc = health_correct / total_samples\n",
    "    \n",
    "    return avg_loss, plant_acc, health_acc\n",
    "\n",
    "def val_epoch_multi(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    plant_correct = 0\n",
    "    health_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y_plant, y_health in tqdm(loader, desc=\"Val - Multi\"):\n",
    "            x, y_plant, y_health = x.to(device), y_plant.to(device), y_health.to(device)\n",
    "            \n",
    "            out_plant, out_health = model(x)\n",
    "            loss_plant = criterion(out_plant, y_plant)\n",
    "            loss_health = criterion(out_health, y_health)\n",
    "            loss = loss_plant + loss_health\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            plant_correct += (out_plant.argmax(1) == y_plant).sum().item()\n",
    "            health_correct += (out_health.argmax(1) == y_health).sum().item()\n",
    "            total_samples += y_plant.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    plant_acc = plant_correct / total_samples\n",
    "    health_acc = health_correct / total_samples\n",
    "    \n",
    "    return avg_loss, plant_acc, health_acc\n",
    "\n",
    "def train_epoch_single(model, loader, optimizer, criterion, device, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for x, y in tqdm(loader, desc=\"Eƒüitim - Single\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total_samples += y.size(0)\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(loader), correct / total_samples\n",
    "\n",
    "def val_epoch_single(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Val - Single\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:19:02.513031Z",
     "iopub.status.busy": "2025-08-28T16:19:02.512746Z",
     "iopub.status.idle": "2025-08-28T16:19:02.534666Z",
     "shell.execute_reply": "2025-08-28T16:19:02.534064Z",
     "shell.execute_reply.started": "2025-08-28T16:19:02.513011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 7. MAIN EXPERIMENT FUNCTION\n",
    "# ===============================\n",
    "\n",
    "def run_experiment(data_dir, num_epochs=10, batch_size=32, learning_rate=1e-4):\n",
    "    \"\"\"Complete experiment pipeline\"\"\"\n",
    "    import torch\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"Plant Disease Classification Experiment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Data preparation\n",
    "    print(\"\\nData preparation...\")\n",
    "    df = define_paths(data_dir)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Number of classes: {df['labels'].nunique()}\")\n",
    "    \n",
    "    train_df, val_df, test_df = split_df(df)\n",
    "    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    \n",
    "    # Data transforms - Overfitting √∂nleme i√ßin g√º√ßlendirilmi≈ü augmentation\n",
    "    train_transform = A.Compose([\n",
    "        A.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)),  # Daha agresif crop\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),  # Yeni: Dikey flip\n",
    "        A.RandomRotate90(p=0.3),  # Yeni: 90 derece rotasyon\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),  # Yeni\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),  # G√º√ßlendirildi\n",
    "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.5),  # G√º√ßlendirildi\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),  # Yeni: G√ºr√ºlt√º ekleme\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),  # Yeni: Random erasing\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # Multi-output dataset\n",
    "    multi_dataset_train = PlantMultiOutputDataset(train_df, transform=train_transform)\n",
    "    multi_dataset_val = PlantMultiOutputDataset(val_df, transform=val_transform)\n",
    "    \n",
    "    plant_names = multi_dataset_train.plant_names\n",
    "    status_names = multi_dataset_train.status_names\n",
    "    plant_map = multi_dataset_train.plant_map\n",
    "    status_map = multi_dataset_train.status_map\n",
    "    \n",
    "    print(f\"Plant species: {len(plant_names)}\")\n",
    "    print(f\"Health statuses: {len(status_names)}\")\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader_multi = DataLoader(multi_dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_multi = DataLoader(multi_dataset_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    plant_only_train = PlantOnlyDataset(train_df, plant_map, transform=train_transform)\n",
    "    plant_only_val = PlantOnlyDataset(val_df, plant_map, transform=val_transform)\n",
    "    health_only_train = HealthOnlyDataset(train_df, status_map, transform=train_transform)\n",
    "    health_only_val = HealthOnlyDataset(val_df, status_map, transform=val_transform)\n",
    "    \n",
    "    train_loader_plant = DataLoader(plant_only_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_plant = DataLoader(plant_only_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    train_loader_health = DataLoader(health_only_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader_health = DataLoader(health_only_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    results = {}\n",
    "    \n",
    "    # =====================================\n",
    "    # 1. MULTI-OUTPUT MODEL TRAINING\n",
    "    # =====================================\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"MULTI-OUTPUT MODEL\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    model_multi = MultiOutputModel(len(plant_names), len(status_names)).to(device)\n",
    "    # Overfitting √∂nleme: Weight decay (L2 regularization) eklendi\n",
    "    optimizer_multi = torch.optim.Adam(\n",
    "        model_multi.parameters(), \n",
    "        lr=learning_rate,\n",
    "        weight_decay=1e-4  # L2 Regularization - Overfitting √∂nler\n",
    "    )\n",
    "    # Learning rate scheduler g√º√ßlendirildi\n",
    "    scheduler_multi = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_multi, \n",
    "        mode='min', \n",
    "        factor=0.5,  # Learning rate'i yarƒ±ya indir\n",
    "        patience=3,  # 3 epoch beklenmeden azalt\n",
    "        min_lr=1e-6  # Minimum learning rate\n",
    "    )\n",
    "    \n",
    "    train_losses_multi = []\n",
    "    val_losses_multi = []\n",
    "    val_plant_accs = []\n",
    "    val_health_accs = []\n",
    "    \n",
    "    # Early Stopping - Overfitting √∂nleme\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=5, min_delta=0.001):\n",
    "            self.patience = patience\n",
    "            self.min_delta = min_delta\n",
    "            self.counter = 0\n",
    "            self.best_loss = float('inf')\n",
    "            \n",
    "        def __call__(self, val_loss):\n",
    "            if val_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "                return False  # Devam et\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True  # Dur\n",
    "                return False  # Devam et\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_plant_acc, train_health_acc = train_epoch_multi(\n",
    "            model_multi, train_loader_multi, optimizer_multi, criterion, device, None  # Scheduler'ƒ± manuel √ßaƒüƒ±racaƒüƒ±z\n",
    "        )\n",
    "        \n",
    "        val_loss, val_plant_acc, val_health_acc = val_epoch_multi(\n",
    "            model_multi, val_loader_multi, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler (ReduceLROnPlateau validation loss'a g√∂re √ßalƒ±≈üƒ±r)\n",
    "        scheduler_multi.step(val_loss)\n",
    "        \n",
    "        train_losses_multi.append(train_loss)\n",
    "        val_losses_multi.append(val_loss)\n",
    "        val_plant_accs.append(val_plant_acc)\n",
    "        val_health_accs.append(val_health_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train Plant: {train_plant_acc:.4f} | Train Health: {train_health_acc:.4f}\")\n",
    "        print(f\"Val Plant: {val_plant_acc:.4f} | Val Health: {val_health_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping kontrol√º\n",
    "        if early_stopping(val_loss):\n",
    "            print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1}\")\n",
    "            print(f\"   Best validation loss: {early_stopping.best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    multi_time = time.time() - start_time\n",
    "    \n",
    "    results['multi'] = {\n",
    "        'final_plant_acc': val_plant_acc,\n",
    "        'final_health_acc': val_health_acc,\n",
    "        'train_losses': train_losses_multi,\n",
    "        'val_losses': val_losses_multi,\n",
    "        'val_plant_accs': val_plant_accs,\n",
    "        'val_health_accs': val_health_accs,\n",
    "        'training_time': multi_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMulti-output Model Final Results:\")\n",
    "    print(f\"Plant Accuracy: {val_plant_acc:.4f}\")\n",
    "    print(f\"Health Accuracy: {val_health_acc:.4f}\")\n",
    "    print(f\"Average Accuracy: {(val_plant_acc + val_health_acc)/2:.4f}\")\n",
    "    print(f\"Training Time: {multi_time:.1f}s\")\n",
    "    \n",
    "    # =====================================\n",
    "    # 2. PLANT-ONLY MODEL TRAINING\n",
    "    # =====================================\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"PLANT-ONLY MODEL\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    plant_model = SingleOutputModel(len(plant_names)).to(device)\n",
    "    # Overfitting √∂nleme: Weight decay eklendi\n",
    "    optimizer_plant = torch.optim.Adam(\n",
    "        plant_model.parameters(), \n",
    "        lr=learning_rate,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    scheduler_plant = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_plant, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    train_losses_plant = []\n",
    "    val_losses_plant = []\n",
    "    val_accs_plant = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch_single(\n",
    "            plant_model, train_loader_plant, optimizer_plant, criterion, device, None\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc = val_epoch_single(\n",
    "            plant_model, val_loader_plant, criterion, device\n",
    "        )\n",
    "        \n",
    "        scheduler_plant.step(val_loss)\n",
    "        \n",
    "        train_losses_plant.append(train_loss)\n",
    "        val_losses_plant.append(val_loss)\n",
    "        val_accs_plant.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    plant_time = time.time() - start_time\n",
    "    \n",
    "    results['plant'] = {\n",
    "        'final_accuracy': val_acc,\n",
    "        'train_losses': train_losses_plant,\n",
    "        'val_losses': val_losses_plant,\n",
    "        'val_accs': val_accs_plant,\n",
    "        'training_time': plant_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPlant-only Model Final Results:\")\n",
    "    print(f\"Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Training Time: {plant_time:.1f}s\")\n",
    "    \n",
    "    # =====================================\n",
    "    # 3. HEALTH-ONLY MODEL TRAINING\n",
    "    # =====================================\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"HEALTH-ONLY MODEL\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    health_model = SingleOutputModel(len(status_names)).to(device)\n",
    "    # Overfitting √∂nleme: Weight decay eklendi\n",
    "    optimizer_health = torch.optim.Adam(\n",
    "        health_model.parameters(), \n",
    "        lr=learning_rate,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    scheduler_health = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_health, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    train_losses_health = []\n",
    "    val_losses_health = []\n",
    "    val_accs_health = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch_single(\n",
    "            health_model, train_loader_health, optimizer_health, criterion, device, None\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc = val_epoch_single(\n",
    "            health_model, val_loader_health, criterion, device\n",
    "        )\n",
    "        \n",
    "        scheduler_health.step(val_loss)\n",
    "        \n",
    "        train_losses_health.append(train_loss)\n",
    "        val_losses_health.append(val_loss)\n",
    "        val_accs_health.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    health_time = time.time() - start_time\n",
    "    \n",
    "    results['health'] = {\n",
    "        'final_accuracy': val_acc,\n",
    "        'train_losses': train_losses_health,\n",
    "        'val_losses': val_losses_health,\n",
    "        'val_accs': val_accs_health,\n",
    "        'training_time': health_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nHealth-only Model Final Results:\")\n",
    "    print(f\"Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Training Time: {health_time:.1f}s\")\n",
    "    \n",
    "    # =====================================\n",
    "    # 4. FINAL COMPARISON\n",
    "    # =====================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    multi_avg = (results['multi']['final_plant_acc'] + results['multi']['final_health_acc']) / 2\n",
    "    single_avg = (results['plant']['final_accuracy'] + results['health']['final_accuracy']) / 2\n",
    "    \n",
    "    print(f\"\\nMULTI-OUTPUT MODEL:\")\n",
    "    print(f\"  Plant Accuracy: {results['multi']['final_plant_acc']:.4f}\")\n",
    "    print(f\"  Health Accuracy: {results['multi']['final_health_acc']:.4f}\")\n",
    "    print(f\"  Average Accuracy: {multi_avg:.4f}\")\n",
    "    print(f\"  Training Time: {results['multi']['training_time']:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nSINGLE-OUTPUT MODELS:\")\n",
    "    print(f\"  Plant Accuracy: {results['plant']['final_accuracy']:.4f}\")\n",
    "    print(f\"  Health Accuracy: {results['health']['final_accuracy']:.4f}\")\n",
    "    print(f\"  Average Accuracy: {single_avg:.4f}\")\n",
    "    print(f\"  Combined Training Time: {results['plant']['training_time'] + results['health']['training_time']:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE DIFFERENCE:\")\n",
    "    plant_diff = results['multi']['final_plant_acc'] - results['plant']['final_accuracy']\n",
    "    health_diff = results['multi']['final_health_acc'] - results['health']['final_accuracy']\n",
    "    avg_diff = multi_avg - single_avg\n",
    "    time_saving = (results['plant']['training_time'] + results['health']['training_time']) - results['multi']['training_time']\n",
    "    \n",
    "    print(f\"  Plant Classification: {plant_diff:+.4f}\")\n",
    "    print(f\"  Health Classification: {health_diff:+.4f}\")\n",
    "    print(f\"  Average: {avg_diff:+.4f}\")\n",
    "    print(f\"  Time Saved: {time_saving:.1f}s ({time_saving/(results['plant']['training_time'] + results['health']['training_time'])*100:.1f}%)\")\n",
    "    \n",
    "    if avg_diff > 0:\n",
    "        print(f\"\\nMulti-output model performs {avg_diff:.4f} points better on average!\")\n",
    "    else:\n",
    "        print(f\"\\nSingle-output models perform {abs(avg_diff):.4f} points better on average!\")\n",
    "    \n",
    "    # =====================================\n",
    "    # OTOMATIK MODEL KAYDETME\n",
    "    # =====================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üíæ MODEL KAYDEDƒ∞Lƒ∞YOR...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    bundle = {\n",
    "        \"state_dict\": model_multi.state_dict(),\n",
    "        \"plant_names\": plant_names,\n",
    "        \"status_names\": status_names,\n",
    "        \"plant_output_dim\": len(plant_names),\n",
    "        \"status_output_dim\": len(status_names),\n",
    "        \"img_size\": 224,\n",
    "        \"mean\": [0.485, 0.456, 0.406],\n",
    "        \"std\": [0.229, 0.224, 0.225],\n",
    "    }\n",
    "    \n",
    "    out_path = Path(\"../backend/models/plantvillage_multi.pt\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Eski modeli yedekle (eƒüer varsa)\n",
    "    backup_path = Path(\"../backend/models/plantvillage_multi_backup.pt\")\n",
    "    if out_path.exists():\n",
    "        import shutil\n",
    "        shutil.copy2(out_path, backup_path)\n",
    "        print(f\"üì¶ Eski model yedeklendi: {backup_path}\")\n",
    "    \n",
    "    torch.save(bundle, out_path)\n",
    "    \n",
    "    print(f\"‚úÖ Model ba≈üarƒ±yla kaydedildi: {out_path}\")\n",
    "    print(f\"   Plant classes: {len(plant_names)}\")\n",
    "    print(f\"   Status classes: {len(status_names)}\")\n",
    "    print(f\"   Model size: {out_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return results, model_multi, plant_names, status_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli kaydet\n",
    "from pathlib import Path\n",
    "\n",
    "bundle = {\n",
    "    \"state_dict\": model_multi.state_dict(),\n",
    "    \"plant_names\": plant_names,\n",
    "    \"status_names\": status_names,\n",
    "    \"plant_output_dim\": len(plant_names),\n",
    "    \"status_output_dim\": len(status_names),\n",
    "    \"img_size\": 224,\n",
    "    \"mean\": [0.485, 0.456, 0.406],\n",
    "    \"std\": [0.229, 0.224, 0.225],\n",
    "}\n",
    "\n",
    "out_path = Path(\"backend/models/plantvillage_multi.pt\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(bundle, out_path)\n",
    "\n",
    "print(f\"‚úÖ Model ba≈üarƒ±yla kaydedildi: {out_path.absolute()}\")\n",
    "print(f\"   Plant classes: {len(plant_names)}\")\n",
    "print(f\"   Status classes: {len(status_names)}\")\n",
    "print(f\"   Model size: {out_path.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model kontrol√º\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = Path(\"backend/models/plantvillage_multi.pt\")\n",
    "\n",
    "print(\"üîç Model Kontrol√º\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(\"‚ùå Dosya bulunamadƒ±!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dosya mevcut: {model_path}\")\n",
    "    print(f\"   Boyut: {model_path.stat().st_size / (1024*1024):.2f} MB\\n\")\n",
    "    \n",
    "    bundle = torch.load(model_path, map_location=\"cpu\")\n",
    "    \n",
    "    print(\"üì¶ ƒ∞√ßerik:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Gerekli alanlar\n",
    "    required = [\"state_dict\", \"plant_names\", \"status_names\"]\n",
    "    all_ok = True\n",
    "    \n",
    "    for key in required:\n",
    "        if key in bundle:\n",
    "            val = bundle[key]\n",
    "            if key == \"state_dict\":\n",
    "                print(f\"‚úÖ {key}: {len(val)} katman\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {key}: {len(val)} √∂ƒüe\")\n",
    "                if len(val) > 0:\n",
    "                    print(f\"   ƒ∞lk 3: {val[:3]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {key}: EKSIK!\")\n",
    "            all_ok = False\n",
    "    \n",
    "    # Opsiyonel alanlar\n",
    "    print(\"\\nüìä Ek Bilgiler:\")\n",
    "    for key in [\"plant_output_dim\", \"status_output_dim\", \"img_size\", \"mean\", \"std\"]:\n",
    "        if key in bundle:\n",
    "            print(f\"‚úÖ {key}: {bundle[key]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if all_ok:\n",
    "        print(\"‚úÖ MODEL DOƒûRU KAYDEDƒ∞LMƒ∞≈û!\")\n",
    "    else:\n",
    "        print(\"‚ùå MODELDE SORUN VAR!\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T16:19:20.799930Z",
     "iopub.status.busy": "2025-08-28T16:19:20.799658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant Disease Classification Experiment\n",
      "==================================================\n",
      "Device: cpu\n",
      "\n",
      "Data preparation...\n",
      "Total samples: 54305\n",
      "Number of classes: 38\n",
      "Train: 43444, Val: 5430, Test: 5431\n",
      "Plant species: 14\n",
      "Health statuses: 21\n",
      "\n",
      "==============================\n",
      "MULTI-OUTPUT MODEL\n",
      "==============================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:06:41<00:00,  2.95s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:33<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1100 | Val Loss: 0.1006\n",
      "Train Plant: 0.8788 | Train Health: 0.8035\n",
      "Val Plant: 0.9941 | Val Health: 0.9801\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:06:24<00:00,  2.93s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:31<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4917 | Val Loss: 0.0821\n",
      "Train Plant: 0.9487 | Train Health: 0.9030\n",
      "Val Plant: 0.9965 | Val Health: 0.9786\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:08:07<00:00,  3.01s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:31<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3917 | Val Loss: 0.0594\n",
      "Train Plant: 0.9576 | Train Health: 0.9213\n",
      "Val Plant: 0.9956 | Val Health: 0.9866\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:53<00:00,  2.87s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:28<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3404 | Val Loss: 0.0470\n",
      "Train Plant: 0.9631 | Train Health: 0.9305\n",
      "Val Plant: 0.9978 | Val Health: 0.9884\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:43<00:00,  2.86s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:33<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2903 | Val Loss: 0.0438\n",
      "Train Plant: 0.9695 | Train Health: 0.9395\n",
      "Val Plant: 0.9983 | Val Health: 0.9880\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:22<00:00,  2.84s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:47<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2793 | Val Loss: 0.0429\n",
      "Train Plant: 0.9695 | Train Health: 0.9408\n",
      "Val Plant: 0.9989 | Val Health: 0.9901\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:43<00:00,  2.86s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:26<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2628 | Val Loss: 0.0352\n",
      "Train Plant: 0.9712 | Train Health: 0.9448\n",
      "Val Plant: 0.9985 | Val Health: 0.9908\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:41<00:00,  2.86s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:29<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2475 | Val Loss: 0.0309\n",
      "Train Plant: 0.9738 | Train Health: 0.9481\n",
      "Val Plant: 0.9976 | Val Health: 0.9910\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:54<00:00,  2.87s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:31<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2345 | Val Loss: 0.0343\n",
      "Train Plant: 0.9760 | Train Health: 0.9491\n",
      "Val Plant: 0.9971 | Val Health: 0.9904\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:51<00:00,  2.87s/it]\n",
      "Val - Multi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:33<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2185 | Val Loss: 0.0317\n",
      "Train Plant: 0.9763 | Train Health: 0.9530\n",
      "Val Plant: 0.9982 | Val Health: 0.9915\n",
      "\n",
      "Multi-output Model Final Results:\n",
      "Plant Accuracy: 0.9982\n",
      "Health Accuracy: 0.9915\n",
      "Average Accuracy: 0.9948\n",
      "Training Time: 41391.6s\n",
      "\n",
      "==============================\n",
      "PLANT-ONLY MODEL\n",
      "==============================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:32<00:00,  2.85s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:34<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3756 | Train Acc: 0.8859\n",
      "Val Loss: 0.0211 | Val Acc: 0.9943\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:34<00:00,  2.85s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:35<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1722 | Train Acc: 0.9461\n",
      "Val Loss: 0.0136 | Val Acc: 0.9952\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:42<00:00,  2.86s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:37<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1358 | Train Acc: 0.9566\n",
      "Val Loss: 0.0120 | Val Acc: 0.9965\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:05:11<00:00,  2.88s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:28<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1191 | Train Acc: 0.9622\n",
      "Val Loss: 0.0085 | Val Acc: 0.9972\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:06:28<00:00,  2.94s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:12<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1061 | Train Acc: 0.9667\n",
      "Val Loss: 0.0080 | Val Acc: 0.9982\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:02:18<00:00,  2.75s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:07<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0949 | Train Acc: 0.9704\n",
      "Val Loss: 0.0074 | Val Acc: 0.9985\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [59:14<00:00,  2.62s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:36<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0898 | Train Acc: 0.9721\n",
      "Val Loss: 0.0107 | Val Acc: 0.9967\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:01:20<00:00,  2.71s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:09<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0891 | Train Acc: 0.9715\n",
      "Val Loss: 0.0067 | Val Acc: 0.9978\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [58:29<00:00,  2.58s/it] \n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:08<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0821 | Train Acc: 0.9747\n",
      "Val Loss: 0.0080 | Val Acc: 0.9972\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:03:52<00:00,  2.82s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:49<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0758 | Train Acc: 0.9751\n",
      "Val Loss: 0.0065 | Val Acc: 0.9982\n",
      "\n",
      "Plant-only Model Final Results:\n",
      "Accuracy: 0.9982\n",
      "Training Time: 39904.2s\n",
      "\n",
      "==============================\n",
      "HEALTH-ONLY MODEL\n",
      "==============================\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:04:39<00:00,  2.86s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:06<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6372 | Train Acc: 0.8079\n",
      "Val Loss: 0.1259 | Val Acc: 0.9558\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:02:06<00:00,  2.74s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:09<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3124 | Train Acc: 0.9019\n",
      "Val Loss: 0.0542 | Val Acc: 0.9818\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:35:53<00:00,  4.24s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [17:46<00:00,  6.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2532 | Train Acc: 0.9192\n",
      "Val Loss: 0.0588 | Val Acc: 0.9796\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [2:13:06<00:00,  5.88s/it] \n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:25<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2133 | Train Acc: 0.9318\n",
      "Val Loss: 0.0415 | Val Acc: 0.9869\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:03:37<00:00,  2.81s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:33<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1985 | Train Acc: 0.9359\n",
      "Val Loss: 0.0240 | Val Acc: 0.9928\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:03:39<00:00,  2.81s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:26<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1781 | Train Acc: 0.9431\n",
      "Val Loss: 0.0293 | Val Acc: 0.9917\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:03:31<00:00,  2.81s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:25<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1743 | Train Acc: 0.9431\n",
      "Val Loss: 0.0299 | Val Acc: 0.9902\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:03:25<00:00,  2.80s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:26<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1649 | Train Acc: 0.9457\n",
      "Val Loss: 0.0268 | Val Acc: 0.9923\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:03:31<00:00,  2.81s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:26<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1529 | Train Acc: 0.9506\n",
      "Val Loss: 0.0271 | Val Acc: 0.9910\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eƒüitim - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1358/1358 [1:03:51<00:00,  2.82s/it]\n",
      "Val - Single: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [03:24<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1153 | Train Acc: 0.9634\n",
      "Val Loss: 0.0154 | Val Acc: 0.9956\n",
      "\n",
      "Health-only Model Final Results:\n",
      "Accuracy: 0.9956\n",
      "Training Time: 47136.1s\n",
      "\n",
      "==================================================\n",
      "FINAL COMPARISON\n",
      "==================================================\n",
      "\n",
      "MULTI-OUTPUT MODEL:\n",
      "  Plant Accuracy: 0.9982\n",
      "  Health Accuracy: 0.9915\n",
      "  Average Accuracy: 0.9948\n",
      "  Training Time: 41391.6s\n",
      "\n",
      "SINGLE-OUTPUT MODELS:\n",
      "  Plant Accuracy: 0.9982\n",
      "  Health Accuracy: 0.9956\n",
      "  Average Accuracy: 0.9969\n",
      "  Combined Training Time: 87040.4s\n",
      "\n",
      "PERFORMANCE DIFFERENCE:\n",
      "  Plant Classification: +0.0000\n",
      "  Health Classification: -0.0041\n",
      "  Average: -0.0020\n",
      "  Time Saved: 45648.8s (52.4%)\n",
      "\n",
      "Single-output models perform 0.0020 points better on average!\n",
      "\n",
      "==================================================\n",
      "üíæ MODEL KAYDEDƒ∞Lƒ∞YOR...\n",
      "==================================================\n",
      "üì¶ Eski model yedeklendi: ../backend/models/plantvillage_multi_backup.pt\n",
      "‚úÖ Model ba≈üarƒ±yla kaydedildi: ../backend/models/plantvillage_multi.pt\n",
      "   Plant classes: 14\n",
      "   Status classes: 21\n",
      "   Model size: 42.79 MB\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "LEARNING CURVES SUMMARY\n",
      "==================================================\n",
      "\n",
      "Multi-output Model - Plant Accuracy per Epoch:\n",
      "Epoch 1: 0.9941\n",
      "Epoch 2: 0.9965\n",
      "Epoch 3: 0.9956\n",
      "Epoch 4: 0.9978\n",
      "Epoch 5: 0.9983\n",
      "Epoch 6: 0.9989\n",
      "Epoch 7: 0.9985\n",
      "Epoch 8: 0.9976\n",
      "Epoch 9: 0.9971\n",
      "Epoch 10: 0.9982\n",
      "\n",
      "Multi-output Model - Health Accuracy per Epoch:\n",
      "Epoch 1: 0.9801\n",
      "Epoch 2: 0.9786\n",
      "Epoch 3: 0.9866\n",
      "Epoch 4: 0.9884\n",
      "Epoch 5: 0.9880\n",
      "Epoch 6: 0.9901\n",
      "Epoch 7: 0.9908\n",
      "Epoch 8: 0.9910\n",
      "Epoch 9: 0.9904\n",
      "Epoch 10: 0.9915\n",
      "\n",
      "Plant-only Model - Accuracy per Epoch:\n",
      "Epoch 1: 0.9943\n",
      "Epoch 2: 0.9952\n",
      "Epoch 3: 0.9965\n",
      "Epoch 4: 0.9972\n",
      "Epoch 5: 0.9982\n",
      "Epoch 6: 0.9985\n",
      "Epoch 7: 0.9967\n",
      "Epoch 8: 0.9978\n",
      "Epoch 9: 0.9972\n",
      "Epoch 10: 0.9982\n",
      "\n",
      "Health-only Model - Accuracy per Epoch:\n",
      "Epoch 1: 0.9558\n",
      "Epoch 2: 0.9818\n",
      "Epoch 3: 0.9796\n",
      "Epoch 4: 0.9869\n",
      "Epoch 5: 0.9928\n",
      "Epoch 6: 0.9917\n",
      "Epoch 7: 0.9902\n",
      "Epoch 8: 0.9923\n",
      "Epoch 9: 0.9910\n",
      "Epoch 10: 0.9956\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 8. RUN EXPERIMENT\n",
    "# ===============================\n",
    "\n",
    "# Run the complete experiment\n",
    "# Model otomatik kaydedilecek (Cell 6'daki fonksiyon i√ßinde)\n",
    "data_dir = 'PlantVillage-Dataset/raw/color'\n",
    "results, model_multi, plant_names, status_names = run_experiment(data_dir, num_epochs=10)\n",
    "\n",
    "# Print learning curves summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LEARNING CURVES SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nMulti-output Model - Plant Accuracy per Epoch:\")\n",
    "for i, acc in enumerate(results['multi']['val_plant_accs']):\n",
    "    print(f\"Epoch {i+1}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nMulti-output Model - Health Accuracy per Epoch:\")\n",
    "for i, acc in enumerate(results['multi']['val_health_accs']):\n",
    "    print(f\"Epoch {i+1}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nPlant-only Model - Accuracy per Epoch:\")\n",
    "for i, acc in enumerate(results['plant']['val_accs']):\n",
    "    print(f\"Epoch {i+1}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nHealth-only Model - Accuracy per Epoch:\")\n",
    "for i, acc in enumerate(results['health']['val_accs']):\n",
    "    print(f\"Epoch {i+1}: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
